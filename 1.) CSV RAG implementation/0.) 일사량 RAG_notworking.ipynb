{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c6d6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv(dotenv_path='C:/Users/user/Desktop/RAG-Implementation/CSV RAG implementation/.env')\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02ef5a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78840\n",
      "일자: 01-01 00:00\n",
      "지역: 광주\n",
      "위도: 35.1729\n",
      "경도: 126.8916\n",
      "대기권 밖 일사량: 0\n"
     ]
    }
   ],
   "source": [
    "'''1. INDEXING'''\n",
    "# 1. 문서 로드 (CSV 파일)\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# CSV loader 생성\n",
    "loader = CSVLoader(\n",
    "    file_path='C:/Users/user/Downloads/한국동서발전(주)_대기권 밖 일사량_20240125.csv',\n",
    "    encoding='cp949'\n",
    ")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 데이터 양 확인\n",
    "print(len(docs))\n",
    "\n",
    "# 불러온 첫번째 row 데이터 확인\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ff3ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78840"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''1 INDEXING'''\n",
    "# 2. 문서 SPLIT \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0 # (row 간 overlap은 불필요)\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3e861b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0.dev20260218+cu128\n",
      "True\n",
      "NVIDIA GeForce RTX 5060 Ti\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "# gpu 사용가능여부 확인\n",
    "# cpu 사용시 large csv 파일을 읽을때 오래 걸림\n",
    "\n",
    "'''\n",
    "False일시:\n",
    "1. 기존 버전 삭제:\n",
    "pip uninstall torch torchvision torchaudio\n",
    "\n",
    "2. CUDA 지원 버전 설치 (12.1 버전 기준):\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "'''\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b6a50",
   "metadata": {},
   "source": [
    "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
    "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
    "\n",
    "'''\n",
    "오류 원인 요약\n",
    "PyTorch 2.6 버전부터 torch.load() 기본 옵션이 **weights_only=True**로 바뀌었습니다.\n",
    "기존 .pt 파일은 모델 가중치(state_dict)와 함께 args 객체 (argparse.Namespace) 도 저장되어 있습니다.\n",
    "그런데 Namespace는 pickle로 저장된 Python 객체인데,\n",
    "weights_only=True일 때는 이런 \"비순수\" 객체들을 로드할 수 없습니다.\n",
    "해결 방법은 간단합니다. 아래와 같이 torch.load를 했다면\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    " \n",
    "\n",
    "weights_only 버전에 False를 입력하여 예전처럼 모든 데이터를 읽게 합니다.\n",
    "\n",
    "checkpoint = torch.load(model_path, weights_only=False)\n",
    " \n",
    "\n",
    "그럼 오류가 해결됩니다.\n",
    "'''\n",
    "\n",
    "이런 메세지가 뜬다면 이전 gpu 사용가능여부 확인 코드에서 버전 체크:\n",
    "\n",
    "2.6 이하라면: ex. 2.5.1+cu121\n",
    "PyTorch 최신 버전 2.6.0 이상으로 업데이트 필요함\n",
    "\n",
    "현재 torch uninstall\n",
    "pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "\n",
    "새로운 torch install 자신의 컴퓨터에 맞는 GPU로 설정해야함 ^ 윗코드 참조\n",
    "pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "\n",
    "버전이 2.12.0.... 인데 왜 돌아가는가?\n",
    "2.12.0.dev20260218+cu128\n",
    " │      └─ 2026-02-18 빌드된 개발버전\n",
    " └─ 안정화 버전 기준으로는 2.12 예정\n",
    "\n",
    " dev 뒷 숫자를 보면 알 수 있듯이, 2.6보다 더욱 최신 버전이지만 앞 2.12 부분만 다를뿐**\n",
    "\n",
    "\n",
    "당연한말이겠지만 reload window 해야함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9aa924ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1 INDEXING'''\n",
    "# 3. SPLIT Embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 한글 추천 모델\n",
    "# 1. jhgan/ko-sroberta-multitask\n",
    "# 2. BAAI/bge-m3\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"jhgan/ko-sroberta-multitask\",\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    ")\n",
    "\n",
    "# 4. VectorDB Store\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 20} #검색 결과 20개로 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec4c0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2. Query'''\n",
    "# LLM 모델 설정: huggingface free 한국어 model 활용\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# HuggingFace Model implementation\n",
    "# repo_id is a registered model inside HuggingFace\n",
    "repo_id = \"google/gemma-2-9b-it\"\n",
    "\n",
    "llm_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "\n",
    "chat_llm = ChatHuggingFace(llm=llm_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19e69541",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2. Query'''\n",
    "# 쿼리단계 프롬프트 템플릿 설정\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt implementation\n",
    "template = \"\"\"당신은 질문-답변을 도와주는 AI 어시스턴트입니다. \n",
    "아래의 제공된 문맥(Context)을 활용해서만 질문에 답하세요. \n",
    "답을 모른다면 모른다고 말하고, 직접적인 답이 문맥에 없다면 문맥을 바탕으로 추론하지 마세요.\n",
    "답변은 반드시 한국어로 작성하세요.\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d38347a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''3. RAG 체인'''\n",
    "# 3. RAG 체인 생성\n",
    "# 검색기(retriever) -> 컨텍스트 전달 -> 프롬프트 적용 -> 모델 실행 -> 결과 파싱\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chat_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff0b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 광주 1월 1일의 일사량을 모두 구한 이후에 평균을 구해줘\n",
      "------------------------------\n",
      "답변: 문맥에서 광주 1월 1일의 대기권 밖 일사량은 2375323 입니다. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1 실행 테스트\n",
    "question = \"광주 1월 1일의 일사량을 모두 구한 이후에 평균을 구해줘\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"질문: {question}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"답변: {response}\")\n",
    "# 틀린 답변이 나옴: 평균 일사량은 1674376.8정도의 값이 나와야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 광주 1월 1일의 일사량을 모두 찾아줘\n",
      "------------------------------\n",
      "답변: * 일자: 01-01 01:00\\n지역: 광주\\n위도: 35.1729\\n경도: 126.8916\\n대기권 밖 일사량: 0\n",
      "* 일자: 01-01 21:00\\n지역: 광주\\n위도: 35.1729\\n경도: 126.8916\\n대기권 밖 일사량: 0 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1 실행 테스트\n",
    "question = \"광주 1월 1일의 일사량을 모두 찾아줘\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"질문: {question}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"답변: {response}\")\n",
    "# 모두 찾아달라한거와 다른 답변이 나옴: 두개의 시간만 알아냄"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
