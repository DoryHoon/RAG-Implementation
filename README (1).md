# LangChain & HuggingFace를 활용한 RAG 챗봇

---

## 1. LangChain이란?

LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 단순화하기 위해 만들어진 오픈소스 프레임워크입니다. 직접 API를 호출하고 프롬프트, 검색기, 출력 파서 등을 수동으로 연결하는 대신, LangChain은 문서 로더, 텍스트 분할기, 벡터 저장소, 프롬프트 템플릿, 체인 등 표준화된 컴포넌트들을 제공하여 이를 손쉽게 조합해 엔드-투-엔드 파이프라인을 구성할 수 있게 해줍니다.

LangChain의 핵심 강점은 **컴포저빌리티(composability)** 입니다. 각 컴포넌트가 일관된 인터페이스를 따르기 때문에, 임베딩 모델이나 벡터 저장소, LLM을 교체하더라도 나머지 코드를 수정할 필요가 없습니다. 이러한 특성 덕분에 LangChain은 RAG 시스템, 에이전트, 챗봇 등 다양한 LLM 기반 애플리케이션을 구축하는 데 표준적으로 활용되는 프레임워크입니다.

---

## 2. HuggingFace란?

HuggingFace는 머신러닝 모델을 중심으로 한 플랫폼이자 오픈소스 생태계입니다. LLM, 임베딩 모델, 이미지 모델 등 수만 개의 사전학습된 모델을 호스팅하며, 누구나 다운로드하거나 API를 통해 접근할 수 있습니다.

이 프로젝트에서 HuggingFace는 두 가지 역할을 담당합니다:
- **임베딩:** `all-MiniLM-L6-v2` 문장 변환(sentence-transformer) 모델이 텍스트를 의미 기반 검색을 위한 수치 벡터로 변환합니다.
- **LLM 추론:** `google/gemma-2-9b-it` 모델을 HuggingFace Inference Endpoints를 통해 서빙하여, 로컬 GPU 없이도 강력한 오픈소스 LLM을 사용할 수 있습니다.

---

## 3. RAG 챗봇이란?

### 3-1. RAG가 왜 필요한가?

일반적인 LLM은 특정 시점까지의 고정된 데이터셋으로 학습됩니다. 이로 인해 두 가지 근본적인 문제가 발생합니다:

- **지식의 한계:** 학습 데이터 이후에 발생한 사건, 기사, 문서에 대해 모델은 알 수 없습니다.
- **환각(Hallucination):** 모델이 모르는 내용을 질문받으면, 그럴듯하지만 잘못된 답변을 만들어내는 경향이 있습니다.

**RAG(Retrieval-Augmented Generation, 검색 증강 생성)** 는 질문이 들어올 때마다 외부의 최신 지식 소스에서 관련 정보를 검색해 모델에게 제공함으로써 이 두 가지 문제를 해결합니다. 모델은 학습 중에 기억한 내용에만 의존하는 대신, 실제 출처 문서를 참고하여 답변하도록 지시받습니다. 그 결과 답변의 정확성, 근거, 검증 가능성이 크게 향상됩니다.

### 3-2. RAG 챗봇을 만들기 위해 LangChain과 HuggingFace가 왜 필요한가?

RAG 파이프라인을 처음부터 직접 구현하려면 여러 복잡한 단계가 필요합니다: 문서 수집 및 파싱, 텍스트 청킹, 임베딩 생성, 벡터 저장 및 검색, 프롬프트 구성, LLM 호출 등. 프레임워크 없이 이를 구현하면 방대한 양의 반복 코드와 연결 코드가 필요합니다.

**LangChain**은 이 모든 컴포넌트를 기본으로 제공하며, 무엇보다 이들을 자유롭게 조합할 수 있습니다. 검색 → 프롬프트 → 생성으로 이어지는 전체 파이프라인을 단 몇 줄의 코드로 정의할 수 있습니다.

**HuggingFace**는 가장 핵심적인 두 단계, 즉 임베딩(텍스트를 검색 가능한 벡터로 변환)과 생성(최종 답변 생성)을 담당하는 모델을 제공합니다. HuggingFace가 API를 통해 오픈소스 모델을 호스팅하기 때문에, Gemma와 같은 고성능 모델을 별도의 추론 인프라 없이도 사용할 수 있습니다.

정리하면, LangChain이 **오케스트레이션(파이프라인 구성)** 을 담당하고, HuggingFace가 **모델 백본** 을 제공합니다.

### 3-3. RAG 챗봇의 전체 구조 및 흐름

RAG 파이프라인은 두 단계로 구성됩니다: 지식 베이스를 준비하는 **인덱싱 단계** (최초 1회 실행)와 사용자 질문에 답하는 **쿼리 단계** (매 질문마다 실행)입니다.

```
===============================================
  인덱싱 단계  (지식 베이스 준비)
===============================================

  [소스 문서 / 웹 페이지]
              |
              v
      [문서 로더 (Document Loader)]
              |
              v
      [텍스트 분할기 (Text Splitter)]  -->  텍스트 청크
              |
              v
    [임베딩 모델 (Embedding Model)]  -->  벡터 표현
              |
              v
       [벡터 저장소 (Vector Store)]  -->  인덱싱 및 저장


===============================================
  쿼리 단계  (사용자 질문에 답변)
===============================================

  [사용자 질문]
        |
        +------------------------------+
        v                              v
  [임베딩 모델]                  (원본 질문 유지)
        |
        v
  [벡터 저장소]  -->  상위 k개의 관련 청크 검색
        |
        v
  [프롬프트 템플릿]
    "아래 문맥만을 사용하여 질문에 답하세요.
     문맥: {검색된 청크}
     질문: {사용자 질문}"
        |
        v
       [LLM]
        |
        v
  [최종 답변]
```